# 04 人工ニューラルネットワーク

## 生物ニューロンからパーセプトロン

[人工知能の歴史のところを参照](https://github.com/Kokensha/machine_learning_deep_learning_lessons/blob/master/01_ai_history.md)

### 生物ニューロン（脳神経細胞・シナプス） -> 生物ニューラルネットワーク（動物や人間の神経ネットワーク、脳）

### 人工ニューロン

### 形式ニューロン

### パーセプトロン 

* ローゼンブラット（RosenBlatt）がパーセプトロンを発表、パーセプトロンは形式ニューロンの実装

### 単層パーセプトロン 

線形分離可能な問題に限られています。 ->多層パーセプトロン

## 多層パーセプトロン（MLP:Multi-layer Perceptron）

* 多層フィードフォーワードニューラルネットワークともいう

### 入力層（input layer）

* 入力層と隠れ層が完全結合

### 隠れ層（hidden layer）

* 隠れ層が１つ以上存在する場合をディープ人口ニューラルネットワークと呼ばれることもある

* 隠れ層と出力層が完全結合

### 出力層（output layer）

## 人工ニューラルネットワーク

* 単層人工ニューラルネットワーク

* 多層人工ニューラルネットワーク

## 人工ニューラルネットワークの構造

### 線形変換

### 非線型変換

### 活性化関数（activation function）

* どうして非線型である必要があるのか？

* 層（layer）ごとに活性化関数を固定するのが一般的

#### 活性化関数種類

* シグモイド関数(sigmoid)

* ReLu

### 損失関数（loss function）/目的関数、コスト関数（cost function）

* 人工ニューラルネットワークの学習で、微分可能な関数なんでも良い

* 決めるもの

#### 損失関数の種類

* 交差エントロピー誤差(cross entropy error)：回帰問題によく利用される

{ \displaystyle
交差エントロピー=-\sum_{x}p(x)\log q(x)
}

* 公差エントロピーはどこから来たか？ →

* 二乗和誤差(mean square error)：分類問題によく利用される

## 人工ニューラルネットワークの学習

* 解析的に解く→解析解、数値的に解く→数値解

* 損失関数を最小化　→損失関数のパラメータの微分（勾配）

### ミニバッチ学習

データを一個ずつ学習するのが効率がよくない、100個ずつバッチにして、全体の近似として学習する

### 勾配降下法 (GD:Gradient Descent)

勾配降下法の課題：谷での振動、プラトー（Plateau：高原）での停止、絶壁での反射

* 微分、偏微分、勾配、勾配法、勾配降下法、勾配上昇法

* 学習率（学習係数）:learning rate 

* 確率勾配降下法 (SDG:Stochastic Gradient Descent) 

* モーメンタム（momentum）振動を防ぐ

* ネステロフの加速勾配法（Nestrov's accelerated gradient method）

* AdaGrad

* RMSProp(ヒントンの講義)

* AdaDelta

* Adam

* 自然勾配法

### 最適化 (optimization)

### 1 epoch 学習

### 誤差逆伝播法（一般的に使われている）

* 注意：順伝播（フィードフォーワード：feed forward）は、ニューラルネットワークのアーキテクチャの話で、層がループせず、次の層へ入力するになる子どです。MLPがそれの典型です。フィードフォーワードニューラルネットワークと対照的なのは、RNNのような、順伝播ではないアーキテクチャです。

####  微分の連鎖率


